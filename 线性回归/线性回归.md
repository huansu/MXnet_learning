# 线性回归
---

> * -- 0:线性模型
>> * 开始解释平方损失函数（square loss）之前，我们用自己的话定义一下线性模型：[函数--通过对象属性的线性组合来学习并预测的函数]
>> * 线性回归是经典的线性模型： $f(x_i)=wx_i+b$,模型目的明确，即使得预测结果$f(x_i)$无限接近于真实结果$y_i$。
>> * 对于回归任务，我们通常需要有一个可行的实验估计方法对模型的泛化性能[对于未知的数据(测试集)也应该表现良好的机器能力]进行评估，通常我们在示例中见到最多的就是均方误差：$E(f;D)=\frac{1}{m}\sum_{i=1}^n(f(x_i)-y_i)^2$

> * -- 1:平方损失函数
>> * 0中我们了解到了线性回归的代数表示模型，通俗一点讲，要想$f(x_i)$的结果最真实，那么$w$和$b$的值就应该最恰当才行。那么如何确定两个未知数的值呢？根据欧氏距离求解的最小二乘法来求解，可得到$w$和$b$的最优解的闭式解。详解可以参考西瓜书的3.2节。它反映了模型学习的状况。

> * -- 2:小批量随机梯度下降
>> * 阐述小批量随机梯度下降之前，我们需要解决三个问题：
>>> * ①什么是梯度下降？
>>> * ②什么是批量梯度下降？
>>> * ③梯度下降的作用是啥？
>>>> * ①梯度的本意是一个向量（矢量），表示某一函数在该点处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向（此梯度的方向）变化最快，变化率最大（为该梯度的模）。理解了梯度，那么梯度下降就是手到擒来，即梯度的反向变化。尝试着想一下，已知$y=x^2$的梯度，你怎么寻找它的最小值呢？很容易想到反向计算梯度，顺着往下走不就找到了吗。可能在举例的$y=x^2$的式子中，很容易找到全局最优。但是对于一些非规则模型，极值和鞍点很容易使模型陷入局部最优，这时候就可以考虑带冲量的梯度下降。
>>>> * ②一次性把全部数据放入算法中进行计算，即进行批量梯度下降，但是，对于这个数据爆炸的时代来说，计算速度难以达到要求，这时就需要用到小批量梯度下降，虽然现在大多数时候提及它都被等同于随机梯度下降，但严格来说是有区分的。小批量梯度下降即解决了批量下降消耗大量时间的问题，又一定程度上解决了随机梯度下降的随机性带来的模型不收敛的问题。选一批数据出来进行梯度下降，保证了模型大方向上下降，又弥补了批量下降和随机梯度下降时间消耗和模型收敛的不足
>>>> * ③ 寻找最优的$w$,而最优的$w$关乎着模型的有效性，这就串联起来了。




